# Scanntech RAG System - ISLR Technical Assistant

![Banner](https://img.shields.io/badge/Status-Completed-success) ![Python](https://img.shields.io/badge/Python-3.10%2B-blue) ![Stack](https://img.shields.io/badge/Stack-Llama3%20%7C%20LangChain%20%7C%20Streamlit-orange) ![RAGAS](https://img.shields.io/badge/Evaluation-RAGAS-red)

> **Repositorio oficial desarrollado para el Scanntech AI Engineer Challenge.**

## ğŸš€ Resumen del Proyecto

Este sistema nace de la necesidad de transformar el libro *"An Introduction to Statistical Learning" (ISL)* en un asistente tÃ©cnico inteligente. El objetivo principal fue construir un ecosistema que no solo responda preguntas tÃ©cnicas con precisiÃ³n, sino que ofrezca **transparencia total** sobre las fuentes consultadas y mantenga un **control de calidad riguroso** mediante mÃ©tricas automatizadas.

A lo largo del desarrollo, implementÃ© una arquitectura basada en tres pilares:
* **RecuperaciÃ³n SemÃ¡ntica Avanzada:** Uso de un flujo de dos pasos (Retrieval + Re-ranking) para asegurar que el modelo trabaje solo con la informaciÃ³n mÃ¡s pertinente.
* **Observabilidad en Tiempo Real:** IntegraciÃ³n de un panel de mÃ©tricas RAGAS directamente en la interfaz para monitorear la salud del sistema.
* **Ingesta con PreservaciÃ³n TÃ©cnica:** Enfoque en la extracciÃ³n limpia de fÃ³rmulas matemÃ¡ticas y estructuras jerÃ¡rquicas del PDF original.

---

## ğŸ“¸ Evidencias del Sistema (Visual Showcase)

### 1. El Orquestador Central (CLI)
DesarrollÃ© un punto de entrada Ãºnico (`main.py`) que permite gestionar todo el ciclo de vida del dato: desde la ingesta inicial hasta la ejecuciÃ³n de benchmarks de evaluaciÃ³n.
![MenÃº Principal CLI](assets/cli_menu.png)

### 2. Dashboard de Usuario y Observabilidad
La interfaz en Streamlit prioriza la experiencia del usuario. El panel lateral muestra los promedios histÃ³ricos de las mÃ©tricas de calidad, permitiendo validar la confianza del sistema antes de iniciar el chat.
![Streamlit Dashboard](assets/sidebar_observability.gif)

### 3. Rendimiento y MÃ©tricas RAGAS
El sistema genera reportes visuales tras cada evaluaciÃ³n masiva, facilitando la identificaciÃ³n de puntos de mejora en la fidelidad y relevancia de las respuestas.
![MÃ©tricas de Rendimiento](assets/ragas_metrics.png)

---

## ğŸ—ï¸ Arquitectura y Preprocesamiento Avanzado

El nÃºcleo del proyecto reside en cÃ³mo se preparan y recuperan los datos tÃ©cnicos. AquÃ­ detallo las decisiones clave en `src/ingestion.py`, `src/query_rag.py` y `src/evaluator.py`:

### 1. Ingesta Inteligente con PyMuPDF4LLM
Para manejar la complejidad del libro ISLR, utilicÃ© `PyMuPDF4LLM` combinado con lÃ³gica personalizada para:
* **ExtracciÃ³n de FÃ³rmulas y LaTeX:** A diferencia de otros extractores, este flujo preserva la sintaxis matemÃ¡tica, permitiendo que el modelo comprenda las ecuaciones sin errores de caracteres extraÃ±os.
* **AnÃ¡lisis de Estructura JerÃ¡rquica:** El sistema mapea la Tabla de Contenidos (TOC) para inyectar metadatos de `CapÃ­tulo`, `SubcapÃ­tulo` y `SecciÃ³n` en cada fragmento.
* **Limpieza Especializada:** Se eliminan ruidos de ediciÃ³n (DOIs, copyright) que suelen ensuciar los embeddings.

### 2. RecuperaciÃ³n de Dos Pasos (Two-Pass Retrieval)
* **BÃºsqueda Vectorial (Broad Search):** RecuperaciÃ³n inicial de 15 fragmentos usando `nomic-ai/nomic-embed-text-v1.5`.
* **Re-ranking SemÃ¡ntico (Deep Search):** AplicaciÃ³n de un **Cross-Encoder** (`ms-marco`) para re-evaluar la relevancia de esos 15 fragmentos, filtrando cualquier contexto que no aporte valor real antes de enviarlo al LLM.

### 3. EvaluaciÃ³n y SanitizaciÃ³n TÃ©cnica
Para garantizar la fiabilidad de las mÃ©tricas de **RAGAS**, implementÃ© un flujo de evaluaciÃ³n especializado:
* **Juez Especializado:** Se utiliza `llama3.1:8b` como juez evaluador por su capacidad superior para seguir instrucciones complejas en comparaciÃ³n con modelos mÃ¡s pequeÃ±os.
* **SanitizaciÃ³n de Datos:** DesarrollÃ© una lÃ³gica que convierte bloques de cÃ³digo y fÃ³rmulas complejas en tokens simplificados (`[MATH_BLOCK]`) antes de la evaluaciÃ³n. Esto evita que el juez se distraiga con la sintaxis de LaTeX y se enfoque puramente en la fidelidad semÃ¡ntica de la respuesta.
* **MÃ©tricas Core:** El sistema mide continuamente *Faithfulness*, *Answer Relevancy*, *Context Precision* y *Context Recall*.

---

## ğŸ“º Demos de InteracciÃ³n

### InteracciÃ³n TÃ©cnica
El asistente explica conceptos estadÃ­sticos complejos citando la ubicaciÃ³n exacta en el libro para su verificaciÃ³n.
![Demo Pregunta VÃ¡lida](assets/valid_query_demo.gif)

### Manejo de Preguntas Fuera de Dominio
El sistema identifica consultas que no pertenecen al dominio del libro (como cultura general), evitando alucinaciones y manteniendo el enfoque tÃ©cnico.
![Demo Pregunta InvÃ¡lida](assets/invalid_query_demo.gif)

---

## ğŸ› ï¸ GuÃ­a de InstalaciÃ³n y Setup

### 1. Modelos de IA (Ollama)
Este sistema utiliza **Ollama** para la inferencia local. AsegÃºrate de tener instalados los siguientes modelos:
```bash
ollama pull llama3.2:3b  # Para el Chat (Velocidad)
ollama pull llama3.1:8b  # Para el Juez Evaluador (Razonamiento)
```

### 2. Entorno y Dependencias
El proyecto utiliza un archivo `requirements.txt` con versiones fijas para garantizar la reproducibilidad del entorno.
```bash
# Crear y activar ambiente (Recomendado Python 3.10 o 3.11)
python -m venv .venv
source .venv/bin/activate  # O .\\.venv\\Scripts\\activate en Windows

# Actualizar pip e instalar dependencias fijas
pip install --upgrade pip
pip install -r requirements.txt
```

---

## ğŸ“‚ Project Structure

```text
scanntech-rag-system/
â”œâ”€â”€ assets/             # ImÃ¡genes y evidencias para el README
â”œâ”€â”€ data/               # Documentos fuente (PDF del libro ISLR)
â”œâ”€â”€ db/                 # Persistencia de ChromaDB (Pre-cargada)
â”œâ”€â”€ eval/               
â”‚   â”œâ”€â”€ benchmark/      # Ground Truth (QA pairs) para evaluaciÃ³n
â”‚   â”œâ”€â”€ logs/           # Trazabilidad de interacciones (JSONL)
â”‚   â””â”€â”€ reports/        # GrÃ¡ficos y CSV generados por RAGAS
â”œâ”€â”€ src/                
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py       # Single Source of Truth (Rutas, Modelos, ConfiguraciÃ³n)
â”‚   â”œâ”€â”€ ingestion.py    # Pipeline ETL (Limpieza, TOC Hierarchy, IndexaciÃ³n)
â”‚   â”œâ”€â”€ query_rag.py    # Motor RAG (Retrieval + Re-ranker + Chain of Verification)
â”‚   â””â”€â”€ evaluator.py    # LÃ³gica de mÃ©tricas RAGAS con sanitizaciÃ³n de texto
â”œâ”€â”€ app.py              # Interfaz de Usuario (Streamlit Dashboard)
â”œâ”€â”€ main.py             # CLI Entrypoint (Orquestador)
â”œâ”€â”€ requirements.txt    # Dependencias del proyecto
â””â”€â”€ README.md           # DocumentaciÃ³n oficial
```

---

## âš¡ Execution Flow

El proyecto cuenta con un `main.py` que centraliza todas las operaciones.

### Paso 1: Iniciar el Orquestador
Ejecuta el siguiente comando en tu terminal:

```bash
python main.py
```

### Paso 2: Seleccionar OperaciÃ³n
VerÃ¡s un menÃº interactivo con las siguientes opciones:

1. **ğŸ› ï¸ INGESTA:** Procesa el PDF `GenAI Challenge.pdf` y crea/actualiza la base de datos vectorial en `db/chroma_db_storage`.
    * *Nota: Se incluye una versiÃ³n pre-cargada de la DB en el repo para pruebas rÃ¡pidas.*
2. **ğŸ“Š EVALUACIÃ“N:** Ejecuta el benchmark de RAGAS. Compara las respuestas del sistema contra el `ground_truth.json` y genera un reporte en CSV.
3. **ğŸ’¬ CHAT:** Lanza automÃ¡ticamente la interfaz web de Streamlit.


### Alternativa: Lanzamiento Directo
Si ya tienes la base de datos y quieres ir directo al chat:

```bash
streamlit run app.py
```

---

## ğŸ›¡ï¸ License & Contact

Desarrollado por [**Jose Luis Cabrera Vega**](https://www.linkedin.com/in/josecabrerav) para el proceso de selecciÃ³n de **Scanntech**.
